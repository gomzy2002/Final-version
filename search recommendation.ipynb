{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jUYDhhzVBMlq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "author_data = pd.read_csv(\"quillverse author data.csv\")\n",
        "blog_rating = pd.read_csv(\"quillverse blog data.csv\")\n",
        "medium_blog_data = pd.read_csv(\"quillverse blog likes.csv\")\n",
        "#print(author_data)\n",
        "#print(blog_rating)\n",
        "#print(medium_blog_data)\n",
        "author_data.head()\n",
        "blog_rating.head()\n",
        "medium_blog_data.head()"
      ],
      "metadata": {
        "id": "fruPonoYyKBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"file1.csv\", \"file2.csv\", and \"file3.csv\" with your actual filenames\n",
        "file_paths = [\"quillverse author data.csv\", \"quillverse blog data.csv\", \"quillverse blog likes.csv\"]\n",
        "\n",
        "#Load each CSV file into separate DataFrames\n",
        "df_list = []\n",
        "for file_path in file_paths:\n",
        "  df = pd.read_csv(file_path)\n",
        "  df_list.append(df)\n",
        "\n",
        "#Choose the merging method\n",
        "# Example: Merge based on a common column \"id\" (ensure all DataFrames have this column)\n",
        "merged_df = pd.merge(df_list[1], df_list[2], on=\"blog_id\", how=\"outer\")  # Join on \"id\" column, outer join keeps all rows\n",
        "merged_df = pd.merge(merged_df, df_list[0], on=\"author_id\", how=\"outer\")  # Join the third DataFrame\n",
        "\n",
        "# Choose the appropriate \"how\" parameter:\n",
        "#   - \"inner\": Keep only rows with matches in both DataFrames (default)\n",
        "#   - \"outer\": Keep all rows from both DataFrames\n",
        "#   - \"left\": Keep all rows from the left DataFrame and matching rows from the right\n",
        "#   - \"right\": Keep all rows from the right DataFrame and matching rows from the left\n",
        "\n",
        "merged_df.info()"
      ],
      "metadata": {
        "id": "fFrhT0ZU0Liv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing/dropping unnecessary columns\n",
        "merged_df_dropped = merged_df.drop(['blog_img'], axis=1)\n",
        "merged_df_dropped.head()\n",
        "#merged_df_dropped.info()\n",
        "#merged_df_dropped.isnull().sum()"
      ],
      "metadata": {
        "id": "jNgSwh7Y6VB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying info about first blog's content\n",
        "merged_df_dropped.head(1)['topic']"
      ],
      "metadata": {
        "id": "jfHOAn6ecTig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For creating a recommendation engine, for each and every blog, we need to create a vector matrix\n",
        "#b'coz while applying recommendation system that usually is based on PAIR-WISE similarity\n",
        "\n",
        "#The \"blog_content\" column is a sentence, a string, so our model cannot understand a sentence,\n",
        "#so for this we'll be using a NLP concept called TF-IDF(help us to create document matrix from this sentences)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfv = TfidfVectorizer(min_df=3, max_features=None,\n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w(1,)',\n",
        "            ngram_range=(1, 3),\n",
        "            stop_words = 'english') #This piece of code removes all the unnecessary characters like is, the, a, a comma or fullstops, etc. which are not required\n",
        "\n",
        "#Filling NoNs with empty strings\n",
        "merged_df_dropped['topic'] = merged_df_dropped['topic'].fillna('')"
      ],
      "metadata": {
        "id": "osFdfjoEc2tU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit Transform\n",
        "# Create a TfidfVectorizer object with a lower min_df value #-->ERROR PARAMETER\n",
        "tfv = TfidfVectorizer(min_df=1)\n",
        "\n",
        "#Converting into Sparse Matrix(a matrix having a lot of zero values, and very less no. of non-zero values.)\n",
        "tfv_matrix = tfv.fit_transform(merged_df_dropped['topic'])\n",
        "tfv_matrix\n",
        "tfv_matrix.shape #it shows(no. of records, no. of features)"
      ],
      "metadata": {
        "id": "1XwQizXyfnOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import sigmoid_kernel\n",
        "#Sigmoid curve, transforming the input between 0 and 1\n",
        "\n",
        "sig = sigmoid_kernel(tfv_matrix, tfv_matrix) #how summary 1 is related to summary 1-->this value will be high!\n",
        "sig[0]  #Dive deeper into this concept for more understanding"
      ],
      "metadata": {
        "id": "FljiMenegwAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reverse mapping of indices and blog titles\n",
        "indices = pd.Series(merged_df_dropped.index, index=merged_df_dropped['blog_title'])\n",
        "indices"
      ],
      "metadata": {
        "id": "ZYowrfpKCoXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices['Rights of a Daughter to Ancestral Property']"
      ],
      "metadata": {
        "id": "sXI37etxGJ9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig[13]"
      ],
      "metadata": {
        "id": "Eae2yyIuGivc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(enumerate(sig[indices['Rights of a Daughter to Ancestral Property']]))"
      ],
      "metadata": {
        "id": "heBAfJm3GvPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(list(enumerate(sig[indices['Rights of a Daughter to Ancestral Property']])), key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "hz4yeSsdG93H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Last 4 set of codes will be included in a function that we'll create for our Recommendation System\n",
        "def give_rec(title, sig=sig):\n",
        "  idx = indices[title]   #Get the indices corresponding to blog_title\n",
        "  sig_scores = list(enumerate(sig[idx]))    #Get pairwise similarity score\n",
        "  sig_scores =sorted(sig_scores, key = lambda x: x[1], reverse=True)   #Sort the blogs\n",
        "  sig_scores = sig_scores[1:7]       #Scores of 6 most similar blogs\n",
        "  blog_indices = [i[0] for i in sig_scores]      #Blog indices\n",
        "  return merged_df_dropped['blog_title'].iloc[blog_indices]        #Top 10 most similar blogs"
      ],
      "metadata": {
        "id": "kZYAGkKvHVM9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now testing our RS\n",
        "give_rec('Role of Productivity Apps')"
      ],
      "metadata": {
        "id": "ixfHgMa3JCc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}